# whatever2vec ü§∑‚Äç‚ôÇÔ∏è

By @christabella, @NightmareNyx and @smspillaz

We present a comprehensive comparison of word vector generation models using the skip-gram algorithm, the continuous bag of words algorithm (Mikolov et al.), the AWD-LSTM model (Merity et al.) and Node2Vec over a graph of words (Grover et al.). We also compare word senses using the approach given in Arora et al.

The results of our experiments can be found in the [results](https://github.com/smspillaz/whatever2vec/tree/master/results) directory.

## Running tests on word vectors

Once word vectors have been trained, you can get the benchmark results on word-benchmarks by using:

    pushd submodules/word-benchmarks/tests
    METHOD=some-label ./run-tests ../../PATH/TO/GENSIM/VECTORS
    popd

The results should be put into `results/some-label`

## Results

### LM2Word2Vec

#### Analogy

| model  | google-analogies.csv | jair.csv | msr.csv    | sat.csv    | semeval.csv | 
|--------|----------------------|----------|------------|------------|-------------| 
| 100.vw | 0.6403398            | 0        | 0.6013075  | 0.25918242 | 0.30312118  | 
| 150.vw | 0.64039266           | 0        | 0.60108894 | 0.25918135 | 0.30323598  | 
| 200.vw | 0.5558439            | 0        | 0.52386457 | 0.22404711 | 0.24454139  | 
| 250.vw | 0.5342923            | 0        | 0.49946675 | 0.20587595 | 0.22498928  | 
| 300.vw | 0.5533238            | 0        | 0.5254152  | 0.20573309 | 0.239287    | 
| 400.vw | 0.47414735           | 0        | 0.45231628 | 0.17319292 | 0.19356392  | 
| 50.vw  | 0.6427122            | 0        | 0.59071636 | 0.30830148 | 0.3142907   | 

#### Clustering

| model  | ap.csv              | battig.csv         | bless.csv          | essli-2008.csv      | 
|--------|---------------------|--------------------|--------------------|---------------------| 
| 100.vw | 0.5111549817629505  | 0.4701409492539494 | 0.609097674518625  | 0.5948063443028617  | 
| 150.vw | 0.48337606690742263 | 0.4726439058488439 | 0.553934936613524  | 0.4713177507990925  | 
| 200.vw | 0.511383587009481   | 0.4992238436761192 | 0.6126116789022762 | 0.5054455414190702  | 
| 250.vw | 0.534971147431235   | 0.4949624355341389 | 0.5926719306594618 | 0.5428752562899452  | 
| 300.vw | 0.4758289068682214  | 0.4697019245667439 | 0.5340228077481495 | 0.45504684670414264 | 
| 400.vw | 0.5413637749205396  | 0.4804087142051286 | 0.6253793226808986 | 0.5574793252585205  | 
| 50.vw  | 0.5144323695914503  | 0.4380701059353572 | 0.5196268914210909 | 0.5470529637342717  | 

#### Outlier Detection

| model  | 8-8-8.csv | wikisem500.csv     | 
|--------|-----------|--------------------| 
| 100.vw | 0.78125   | 0.5843355119825708 | 
| 150.vw | 0.78125   | 0.5848023653906007 | 
| 200.vw | 0.890625  | 0.578145813881108  | 
| 250.vw | 0.671875  | 0.5966472144413321 | 
| 300.vw | 0.890625  | 0.5511367880485527 | 
| 400.vw | 0.890625  | 0.5744026221599752 | 
| 50.vw  | 0.734375  | 0.6143921179582945 | 

#### Similarity

| model  | mc-30.csv           | men.csv             | mturk-287.csv       | mturk-771.csv       | rg-65.csv           | rw.csv              | semeval17.csv       | simverb-3500.csv   | verb-143.csv        | wordsim353-rel.csv  | wordsim353-sim.csv  | yp-130.csv          | 
|--------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|--------------------|---------------------|---------------------|---------------------|---------------------| 
| 100.vw | 0.23262541544040044 | 0.1987571170169611  | 0.2299824983457101  | 0.23243278729087308 | 0.24639078094409064 | 0.26130866001826786 | 0.21347329613235264 | 0.7104533505530943 | 0.29282450701225365 | 0.2349569548387237  | 0.16924773826374084 | 0.2913383059767576  | 
| 150.vw | 0.23345235339403153 | 0.19875746533272165 | 0.229978757863782   | 0.2322595934252116  | 0.2456465694996027  | 0.2613362571071438  | 0.21357433823017793 | 0.7102285577319915 | 0.29339505643267483 | 0.23503139389656919 | 0.16901092648445346 | 0.2919843622524005  | 
| 200.vw | 0.26101349561015763 | 0.24530788521412764 | 0.3517292108064354  | 0.3268703272746958  | 0.2492108909143851  | 0.3132374001824417  | 0.23401217016287978 | 0.7575145062073669 | 0.279708762641937   | 0.3188833066455769  | 0.24707455361022482 | 0.2816417728634981  | 
| 250.vw | 0.2688238610188166  | 0.25876522520390655 | 0.3633188989739486  | 0.3423926644681169  | 0.25916253110766413 | 0.32137162352681575 | 0.2482190000116365  | 0.7717744624336146 | 0.27800052317716767 | 0.33567863427891453 | 0.2678640785343489  | 0.28081431128428536 | 
| 300.vw | 0.25841380699078237 | 0.2503854629996047  | 0.3061668968330241  | 0.31148313830086743 | 0.25706328180203075 | 0.28782788833692075 | 0.24473696922308505 | 0.7548047950251773 | 0.281380884222095   | 0.3092829964800793  | 0.23902553946920194 | 0.28390958477373307 | 
| 400.vw | 0.2860361686358848  | 0.2847717287496975  | 0.3803760932231476  | 0.3766193355413912  | 0.2763803583177236  | 0.3354976072341171  | 0.27245529542688596 | 0.7979207980398926 | 0.2717867930499571  | 0.3620669955474114  | 0.2956181045926499  | 0.27530058174236466 | 
| 50.vw  | 0.28006486112276713 | 0.23680829711709173 | 0.33807099371004945 | 0.28372421405925496 | 0.2554075000068316  | 0.3105925488978037  | 0.22681946402599595 | 0.7063345005552424 | 0.3300705308093438  | 0.26734758736550057 | 0.2216020986003687  | 0.3316088645703517  | 
